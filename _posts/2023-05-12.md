---
layout: distill
title: Neural Path Planning
description: Neural ODEs, Signed Distance Fields, and how they naturally prevent robots from bumping into each other.
date: 2025-06-24
authors:
  - name: Progyan Das
    url: "https://progyan.me"
    affiliations:
      name: Indian Institute of Technology, Gandhinagar
bibliography: 2025-06-24-fourier-flow.bib
toc:
  - name: The Dance Floor Problem
  - name: From Discrete Steps to Continuous Flows
  - name: The Magic of Ordinary Differential Equations
  - name: When Time Becomes Space
  - name: Making Walls Repulsive
  - name: The Fourier Touch
  - name: Putting It All Together
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## The Dance Floor Problem

Picture this: you're at a wedding, and the dance floor is packed. Everyone needs to get from where they are to somewhere else – maybe to congratulate the couple, grab a drink, or find their table. Now, imagine if everyone just walked in straight lines to their destinations. Chaos, right? People would bump into each other, step on toes, and generally create a mess.

This is essentially the multi-agent path planning problem. We have multiple "agents" (think robots, drones, or even virtual characters in a game) that need to move from point A to point B without colliding with each other or with obstacles. It sounds simple, but it's one of those problems that gets exponentially harder as you add more agents.

<!-- ADD ANIMATION OF AGENTS COLLIDING HERE -->

Traditional approaches to this problem are like giving each dancer a grid on the floor and saying "you can only step on these squares." They discretize space and time, creating a chess-board-like representation where agents hop from square to square. But real movement isn't like that – it's smooth, continuous, like water flowing around rocks.

What if, instead of forcing our agents to hop between discrete points, we could describe their motion as a continuous flow? What if we could guarantee, mathematically, that they'd never collide?

That's where our journey begins.

## From Discrete Steps to Continuous Flows

Let me share a realization that changed how I think about motion planning. Most path planning algorithms are like trying to approximate a smooth curve with a series of straight line segments. Sure, if you use enough segments, it looks smooth from a distance, but zoom in and you see the jagged edges.

<!-- CODE 1: Simple discretized path vs continuous path visualization -->
```python
import numpy as np
import matplotlib.pyplot as plt

# Discrete path
t_discrete = np.linspace(0, 1, 10)
x_discrete = np.sin(2 * np.pi * t_discrete)
y_discrete = np.cos(2 * np.pi * t_discrete)

# Continuous path
t_continuous = np.linspace(0, 1, 1000)
x_continuous = np.sin(2 * np.pi * t_continuous)
y_continuous = np.cos(2 * np.pi * t_continuous)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(x_discrete, y_discrete, 'o-', markersize=8, linewidth=2)
plt.title('Discrete Path (10 points)')
plt.axis('equal')

plt.subplot(1, 2, 2)
plt.plot(x_continuous, y_continuous, linewidth=2)
plt.title('Continuous Path')
plt.axis('equal')
plt.tight_layout()
plt.show()
```

The discrete approach has fundamental limitations:
1. **Resolution dependence**: Too coarse, and you miss valid paths. Too fine, and computation explodes.
2. **Jerky motion**: Real robots can't teleport between grid points.
3. **Collision checking nightmares**: You have to check every segment for every agent at every timestep.

But what if we thought about motion differently? Instead of asking "what sequence of positions should the agent visit?", what if we asked "what velocity should the agent have at each point in space and time?"

This shift in perspective is profound. Instead of a sequence of waypoints, we're now thinking about a *velocity field* – at every point in space, we specify a velocity vector that tells an agent which direction to move and how fast.

$$ \frac{d\mathbf{x}}{dt} = \mathbf{v}(\mathbf{x}, t) $$

This is an ordinary differential equation (ODE), and it's about to become our best friend.

## The Magic of Ordinary Differential Equations

Here's where things get beautiful. ODEs have a property that seems almost too good to be true: uniqueness of solutions.

Let me explain with an analogy. Imagine you're floating down a river. The river's current at each point determines where you'll go next. If two leaves start at different positions on the river, can they ever end up at exactly the same spot at exactly the same time?

The answer, surprisingly, is no – not if the river's flow is "well-behaved" (what mathematicians call Lipschitz continuous). This is the Picard-Lindelöf theorem, and it's the cornerstone of our approach.

<!-- ADD VISUALIZATION OF TWO TRAJECTORIES IN A FLOW FIELD -->

Think about what this means for our robots. If each robot follows the same velocity field $$ \mathbf{v}(\mathbf{x}, t) $$, and they start at different positions, they can never collide! The mathematics guarantees it.

Let's make this concrete. Suppose we have a neural network that takes a position $$ \mathbf{x} $$ and time $$ t $$, and outputs a velocity $$ \mathbf{v} $$:

```python
class VelocityField(nn.Module):
    def __init__(self, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3, hidden_dim),  # x, y, t
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 2)   # vx, vy
        )
    
    def forward(self, x, y, t):
        inputs = torch.stack([x, y, t], dim=-1)
        return self.net(inputs)
```

Each robot's trajectory is then the solution to:
$$ \frac{d\mathbf{x}_i}{dt} = \mathbf{v}(\mathbf{x}_i(t), t) $$

Starting from $$ \mathbf{x}_i(0) = \mathbf{x}_i^0 $$.

The beauty is that we don't explicitly program collision avoidance – it emerges naturally from the mathematical structure of ODEs. It's like how water molecules in a river don't "decide" to avoid each other; they simply can't occupy the same space.

## When Time Becomes Space

But wait, there's a catch. The uniqueness theorem is almost *too* strong. It says trajectories can never intersect, period. But what if two robots need to use the same narrow corridor, just at different times? Under our current formulation, if Robot A passes through a point, Robot B can never pass through that same point, even hours later!

This is where we pull a clever trick: we augment our state space with time itself.

<!-- ADD DIAGRAM SHOWING 2D SPACE VS 3D SPACE-TIME -->

Instead of thinking about trajectories in 2D space (x, y), we think about them in 3D space-time (x, y, t). In this augmented space, two robots can occupy the same (x, y) position as long as they do so at different times t.

$$ \tilde{\mathbf{x}} = (\mathbf{x}, t) $$

The augmented dynamics become:
$$ \frac{d\tilde{\mathbf{x}}}{dt} = \begin{pmatrix} \mathbf{v}(\mathbf{x}, t) \\ 1 \end{pmatrix} $$

This is genius because:
1. Time always increases (the "1" in the equation above)
2. Trajectories in space-time still can't intersect
3. But trajectories in space alone can cross at different times

It's like adding a vertical time dimension to our dance floor. Two dancers can now use the same spot on the floor, as long as they're at different "heights" in time.

## Making Walls Repulsive

So far, we've solved robot-robot collisions, but what about walls and obstacles? Here's where Signed Distance Functions (SDFs) enter the picture.

An SDF is beautifully simple: at every point in space, it tells you the distance to the nearest obstacle. Positive values mean you're in free space, negative means you're inside an obstacle, and zero means you're exactly on the boundary.

<!-- CODE 2: Visualizing an SDF -->
```python
def create_sdf(obstacle_mask, resolution=1.0):
    """
    Create a signed distance field from a binary obstacle mask
    using the Fast Marching Method
    """
    from skfmm import distance
    
    # Distance from obstacles (positive outside, negative inside)
    sdf = distance(obstacle_mask)
    
    return sdf

# Example: Room with obstacles
room = np.ones((100, 100))
room[40:60, 40:60] = 0  # Square obstacle
room[20:30, 70:90] = 0  # Rectangular obstacle

sdf = create_sdf(room == 0)

# Visualize
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.imshow(room, cmap='binary')
plt.title('Environment')

plt.subplot(1, 2, 2)
plt.imshow(sdf, cmap='RdBu')
plt.colorbar(label='Distance to nearest obstacle')
plt.title('Signed Distance Field')
plt.show()
```

Now comes the elegant part. We can use the SDF to modulate our velocity field:

$$ \mathbf{v}_{\text{safe}}(\mathbf{x}, t) = \mathbf{v}(\mathbf{x}, t) \cdot \max(0, \phi(\mathbf{x})) $$

Where $$ \phi(\mathbf{x}) $$ is the SDF value at position $$ \mathbf{x} $$.

What does this do? As a robot approaches an obstacle (where $$ \phi \to 0 $$), its velocity drops to zero. It's like the obstacles have an invisible force field that slows robots down as they get closer. The robot literally cannot penetrate the obstacle because its velocity becomes zero at the boundary.

But we can be even smarter. The gradient of the SDF, $$ \nabla \phi $$, points directly away from the nearest obstacle. We can use this to make our robots "slide" along walls rather than getting stuck:

$$ \mathbf{v}_{\text{tangent}} = \mathbf{v} - (\mathbf{v} \cdot \nabla \phi) \nabla \phi $$

This projects the desired velocity onto the tangent plane of the obstacle, creating a smooth sliding motion.

<!-- ADD VISUALIZATION OF VELOCITY FIELD AROUND OBSTACLES -->

## The Fourier Touch

Here's where we add the secret sauce that makes everything work beautifully in practice: Fourier features.

You see, neural networks are surprisingly bad at learning high-frequency functions. Try training a simple MLP to fit $$ \sin(100x) $$ and you'll see what I mean – it struggles to capture the rapid oscillations.

But in path planning, we often need sharp turns and quick maneuvers. How do we help our neural network learn these complex velocity fields?

The answer comes from signal processing. Any function can be decomposed into a sum of sine and cosine waves of different frequencies (that's the Fourier transform). So instead of feeding raw positions (x, y, t) to our network, we first encode them using sinusoidal functions:

```python
def fourier_features(x, B):
    """
    Map coordinates to high-dimensional Fourier features
    x: input coordinates [..., d]
    B: frequency matrix [d, num_frequencies]
    """
    x_proj = 2 * np.pi * x @ B
    return np.concatenate([np.sin(x_proj), np.cos(x_proj)], axis=-1)

# Example usage
B = np.random.randn(3, 128)  # 3D input (x,y,t), 128 frequencies
x = np.array([0.5, 0.3, 0.1])  # Some position
features = fourier_features(x, B)  # 256-dimensional feature vector
```

This is like giving our neural network a palette of different frequency "brushes" to paint the velocity field. Low frequencies create smooth, sweeping motions, while high frequencies enable sharp turns and intricate maneuvers.

<!-- ADD COMPARISON OF VELOCITY FIELDS WITH/WITHOUT FOURIER FEATURES -->

The impact is dramatic. Without Fourier features, learned paths tend to be overly smooth, taking wide turns around obstacles. With them, robots can navigate tight spaces and make precise movements when needed.

## Putting It All Together

Let's see how all these pieces fit together into a complete algorithm. The beauty is in how each component addresses a specific challenge:

1. **Neural ODEs** handle the continuous dynamics and guarantee collision-free paths
2. **Time augmentation** allows spatial path crossing at different times  
3. **SDFs** encode obstacles and enable smooth avoidance
4. **Fourier features** give us the expressiveness to learn complex maneuvers

The training process is surprisingly elegant. We don't need to explicitly teach the system about collisions – we just need to specify where each robot should end up:

```python
def train_step(velocity_field, initial_positions, goal_positions, sdf, optimizer):
    # Forward pass: integrate ODEs from t=0 to t=1
    trajectories = odeint(
        lambda x, t: velocity_field(x, t) * torch.clamp(sdf(x), min=0),
        initial_positions,
        torch.linspace(0, 1, 100)
    )
    
    # Loss: did we reach the goals?
    final_positions = trajectories[-1]
    endpoint_loss = torch.mean((final_positions - goal_positions)**2)
    
    # Optional: penalize getting too close to obstacles
    obstacle_penalty = torch.mean(torch.relu(-sdf(trajectories)))
    
    # Optional: encourage smooth paths
    velocity_changes = torch.diff(trajectories, dim=0)
    smoothness_loss = torch.mean(velocity_changes**2)
    
    total_loss = endpoint_loss + 0.1 * obstacle_penalty + 0.01 * smoothness_loss
    
    # Backward pass and update
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()
    
    return total_loss.item()
```

The magic happens during backpropagation. The gradients flow backward through the ODE integration, adjusting the velocity field to better guide robots to their goals. It's like the entire flow field is learning to choreograph the perfect dance.

<!-- CODE 3: Complete training loop -->

## Emerging Behaviors and Surprising Patterns

What fascinates me most about this approach is the emergent behaviors we see. Without explicitly programming any coordination strategies, the system learns sophisticated patterns:

### The Vortex Pattern
When multiple robots need to swap positions in a confined space, the velocity field often develops a vortex pattern. Robots circulate around a central point, like dancers in a round dance, allowing smooth position exchanges.

<!-- ADD VISUALIZATION OF VORTEX PATTERN -->

### The Highway Formation  
In narrow corridors, the field naturally develops "lanes" – streams going in opposite directions, minimizing conflicts. It's remarkably similar to how pedestrians self-organize on busy sidewalks.

### The Waiting Game
Sometimes, the optimal solution involves one robot pausing while another passes. The velocity field learns to create near-zero velocity "pockets" where robots can wait without blocking others.

These aren't hard-coded behaviors – they emerge from the optimization process as natural solutions to the multi-agent coordination problem.

## Challenges and Honest Limitations

Now, I'd be misleading you if I painted this as a perfect solution. Like any approach, it has its challenges:

### Computational Cost
Integrating ODEs, especially with many agents, isn't cheap. Each forward pass requires numerical integration, and backpropagation through the ODE solver adds overhead. For real-time applications with hundreds of agents, this can become a bottleneck.

### Local Minima
The optimization landscape is complex. Sometimes the algorithm finds solutions where a robot takes a very long path to avoid a non-existent conflict, or where robots get "stuck" in equilibrium points of the velocity field.

### Scalability of Expression
As we add more agents, the velocity field needs to become increasingly complex to handle all the interactions. This means more Fourier features, larger networks, and longer training times.

### The Sim-to-Real Gap
In simulation, our ODEs are perfect. In reality, robots have dynamics, delays, and uncertainties. Bridging this gap requires careful controller design and possibly online adaptation.

## Beautiful Mathematics, Practical Impact

Despite these challenges, what excites me about this approach is how it turns a traditionally discrete, combinatorial problem into a smooth, continuous optimization. We're not searching through a tree of possible moves or solving integer programs – we're sculpting a velocity field that guides all agents simultaneously.

The mathematical guarantees are particularly satisfying. In traditional approaches, you might run your algorithm and then check: "Did any robots collide?" With our method, collisions are impossible by construction. It's the difference between hoping your parachute works and knowing it must work because of the laws of physics.

<!-- ADD FINAL VISUALIZATION OF COMPLEX MULTI-AGENT SCENARIO -->

## Looking Forward: The Future of Flow-Based Planning

This work opens several exciting directions:

### Learning from Demonstrations
Instead of specifying goals, could we learn velocity fields from expert demonstrations? Imagine teaching robots to navigate by showing them how humans move through crowds.

### Reactive Planning
Could we make the velocity field responsive to dynamic obstacles? The framework naturally extends to time-varying SDFs and adaptive fields.

### Heterogeneous Agents
What if different robots have different dynamics – some fast, some slow, some large, some small? The velocity field could learn agent-specific behaviors.

### 3D and Beyond
While we focused on 2D navigation, the mathematics works in any dimension. Drone swarms, underwater robots, even joint-space planning for robot arms could benefit from this approach.

## A New Lens for an Old Problem

Path planning is one of those fundamental problems that seems simple until you try to solve it. "Just move from A to B without hitting things" – how hard could it be?

But like many simple-sounding problems, the devil is in the details. Traditional approaches discretize and search, turning smooth motion into a combinatorial puzzle. Our approach offers a different lens: what if we viewed motion as a continuous flow, governed by learnable dynamics that inherently prevent collisions?

It's not just about solving the problem differently – it's about solving a different problem altogether. Instead of asking "what's the best sequence of moves?", we ask "what's the ideal flow that carries all agents to their destinations?"

The answer, it turns out, is both mathematically elegant and practically effective. By combining ideas from dynamical systems (ODEs), computational geometry (SDFs), and modern deep learning (Neural ODEs and Fourier features), we can create a system that plans smooth, collision-free paths for multiple agents simultaneously.

<!-- ADD CLOSING ANIMATION -->

And perhaps most beautifully, the solution isn't just an algorithm – it's a new way of thinking about motion, coordination, and the dance of navigation in shared spaces.

---

*If you're interested in implementing this approach or diving deeper into the mathematics, check out our [code repository](#) and the [full paper](#). The journey from collision to coordination is fascinating, and there's still so much to explore.*

## Code Appendix: A Minimal Implementation

For those who want to experiment, here's a minimal implementation that captures the key ideas:

```python
import torch
import torch.nn as nn
from torchdiffeq import odeint
import numpy as np

class FourierVelocityField(nn.Module):
    def __init__(self, fourier_features=128, hidden_dim=256):
        super().__init__()
        
        # Random Fourier feature frequencies
        self.B = nn.Parameter(torch.randn(3, fourier_features) * 10, 
                             requires_grad=False)
        
        # Neural network for velocity field
        self.net = nn.Sequential(
            nn.Linear(2 * fourier_features, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 2)
        )
        
    def fourier_encoding(self, x):
        # x shape: [..., 3] (x, y, t)
        x_proj = 2 * np.pi * torch.matmul(x, self.B)
        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
    
    def forward(self, t, positions, sdf_fn):
        # positions shape: [n_agents, 2]
        # Augment with time
        t_expanded = t.expand(positions.shape[0], 1)
        state = torch.cat([positions, t_expanded], dim=-1)
        
        # Fourier encoding
        encoded = self.fourier_encoding(state)
        
        # Compute velocity
        velocity = self.net(encoded)
        
        # Apply SDF modulation for obstacle avoidance
        sdf_values = sdf_fn(positions)
        safe_velocity = velocity * torch.clamp(sdf_values, min=0).unsqueeze(-1)
        
        return safe_velocity

def train_multiagent_planner(n_agents, initial_positions, goal_positions, 
                           sdf_fn, n_epochs=1000):
    model = FourierVelocityField()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    
    for epoch in range(n_epochs):
        # Define dynamics for ODE solver
        def dynamics(t, positions):
            return model(t, positions.reshape(n_agents, 2), sdf_fn).flatten()
        
        # Integrate from t=0 to t=1
        t_span = torch.linspace(0, 1, 50)
        trajectories = odeint(dynamics, 
                            initial_positions.flatten(), 
                            t_span)
        
        # Reshape to [time, n_agents, 2]
        trajectories = trajectories.reshape(-1, n_agents, 2)
        
        # Compute losses
        final_positions = trajectories[-1]
        endpoint_loss = torch.mean((final_positions - goal_positions)**2)
        
        # Obstacle avoidance loss
        all_sdf_values = torch.stack([sdf_fn(trajectories[i]) 
                                     for i in range(len(t_span))])
        obstacle_loss = torch.mean(torch.relu(-all_sdf_values))
        
        # Total loss
        loss = endpoint_loss + 0.1 * obstacle_loss
        
        # Optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")
    
    return model, trajectories

# Example usage
n_agents = 5
initial_positions = torch.randn(n_agents, 2)
goal_positions = torch.randn(n_agents, 2)

# Simple SDF function (distance from origin)
def simple_sdf(positions):
    return torch.norm(positions, dim=-1) - 0.5

model, trajectories = train_multiagent_planner(
    n_agents, initial_positions, goal_positions, simple_sdf)
```

This minimal example captures the essence of our approach: Neural ODEs for dynamics, Fourier features for expressiveness, and SDF-based obstacle avoidance. From here, you can extend it with more sophisticated SDF computation, better loss functions, and visualization tools.

The beauty is that despite its simplicity, this code guarantees collision-free paths between agents. No explicit collision checking, no combinatorial search – just smooth, continuous optimization guided by the fundamental properties of differential equations.

## Mathematical Intuitions: Why This Works

For those who want to dig deeper into why this approach provides guarantees, let's build some mathematical intuition.

### The Uniqueness Theorem Visualized

Consider the simple 1D case where two agents move according to:
$$ \frac{dx_1}{dt} = v(x_1, t), \quad \frac{dx_2}{dt} = v(x_2, t) $$

If they collide at time $$ t^* $$, then $$ x_1(t^*) = x_2(t^*) $$. But then both agents are at the same position, experiencing the same velocity field, so their future evolution must be identical:
$$ x_1(t) = x_2(t) \quad \forall t \geq t^* $$

Working backwards, this means they must have started at the same position – a contradiction if they had different initial positions. It's like trying to trace two different rivers back to the same source; if they truly have different sources, they can't merge.

### The Time Augmentation Trick

The brilliance of time augmentation becomes clear when you visualize trajectories in space-time. In regular space, two paths might need to cross:

```
Agent 1: A → X → B
Agent 2: C → X → D
```

Both need to pass through point X. In standard formulation, this is impossible. But in space-time:

```
Agent 1: (A, 0) → (X, 0.3) → (B, 1)
Agent 2: (C, 0) → (X, 0.7) → (D, 1)
```

They pass through X at different times (0.3 vs 0.7), so their space-time trajectories never intersect. It's like adding a time-stamp to every position, ensuring uniqueness.

### The SDF Barrier

The mathematical beauty of SDF-based collision avoidance lies in its continuity. As an agent approaches an obstacle:

$$ \phi(\mathbf{x}) \to 0^+ $$

The modulated velocity becomes:

$$ \mathbf{v}_{\text{safe}} = \mathbf{v} \cdot \phi(\mathbf{x}) \to 0 $$

This creates what's called a "barrier function" in optimization – a smooth way to enforce hard constraints. The agent slows down continuously as it approaches the obstacle, making it mathematically impossible to penetrate the boundary (where $$ \phi = 0 $$).

## The Philosophy of Continuous Planning

Stepping back, this work represents a philosophical shift in how we approach planning problems. Traditional computer science loves discrete structures – graphs, trees, grids. They're easy to reason about, implement, and analyze.

But the physical world is continuous. Motion is smooth. Time flows without ticks. By embracing this continuity rather than discretizing it away, we gain something profound: solutions that are not just correct, but natural.

It reminds me of the transition from digital to analog synthesizers in music. Digital synthesis is precise and controllable, but analog synthesis has a warmth and continuity that's hard to replicate with discrete samples. Similarly, our continuous path planning has a fluidity that discrete methods struggle to achieve.

## Connections to Other Fields

What's fascinating is how this approach connects to other areas:

### Fluid Dynamics
Our velocity fields are essentially incompressible flows. The agents move like particles in a fluid, which opens up connections to computational fluid dynamics and turbulence modeling.

### Optimal Transport
The problem of moving multiple agents efficiently is related to optimal transport theory – how do you move mass from one distribution to another with minimal cost?

### Swarm Intelligence
The emergent behaviors we see mirror those in natural swarms. Birds flocking, fish schooling, ants foraging – they all follow local rules that create global coordination.

### Dynamical Systems
The entire framework is grounded in dynamical systems theory. Concepts like stability, attractors, and bifurcations all have analogues in our planning context.

These connections suggest that the ideas here might extend far beyond robotics, potentially informing how we think about coordination in complex systems generally.

## Final Thoughts: The Art of Mathematical Modeling

What I find most beautiful about this work is how it demonstrates the art of mathematical modeling. We took a hard combinatorial problem (multi-agent path planning) and transformed it into a continuous optimization problem by changing our perspective.

The key insights weren't about better algorithms or more compute power – they were about finding the right mathematical framework. Once we viewed trajectories as solutions to ODEs, collision avoidance came for free. Once we augmented with time, flexible paths became possible. Once we added Fourier features, complex maneuvers emerged naturally.

It's a reminder that sometimes the best way to solve a problem isn't to attack it head-on, but to reframe it entirely. By finding the right lens through which to view the problem, solutions that seemed impossible become not just possible, but elegant.

And that, ultimately, is what drew me to this work. Not just the practical applications (though those are exciting), but the elegance of turning a discrete search into a continuous flow, of replacing explicit constraints with implicit guarantees, of letting mathematics do the heavy lifting so our robots can dance.
