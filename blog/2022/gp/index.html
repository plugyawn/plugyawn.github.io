<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Understanding Gaussian Processes, from scratch. | This is Progyan Das</title> <meta name="author" content="Progyan Das"/> <meta name="description" content="Sparse, Variational, Gaussian, and how they can be combined in one process."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶æ</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://plugyawn.github.io/blog/2022/gp/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <d-front-matter> <script async type="text/json">{
      "title": "Understanding Gaussian Processes, from scratch.",
      "description": "Sparse, Variational, Gaussian, and how they can be combined in one process.",
      "published": "September 25, 2022",
      "authors": [
        {
          "author": "Progyan Das",
          "authorURL": "https://progyan.me",
          "affiliations": [
            {
              "name": "Indian Institute of Technology, Gandhinagar",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <body class="fixed-top-navsticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">This is Progyan Das</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/cv.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1 class="font-weight-bolder">Understanding Gaussian Processes, from scratch.</h1> <p class="font-weight-lighter">Sparse, Variational, Gaussian, and how they can be combined in one process.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3 class="font-weight-lighter">Contents</h3> <div><a href="#brownian-motion">Brownian Motion</a></div> </nav> </d-contents> <h2 id="brownian-motion">Brownian Motion</h2> <p>As with most of Bayesian theory, we start with Bayes‚Äô Rule,</p> \[P(\theta|y) = \frac{P(y|\theta) \cdot P(\theta)}{P(y)}\] <p>Until now, we have taken \(P(x)\) to mean the probability that \(x\) takes on a particular <em>value</em>, but now the game has changed ‚Äì we shall now take \(P(x)\) to mean the probability that we encounter a particular <strong>function</strong>.</p> <p>Indeed, this simple idea is at the heart of what we call the Gaussian Process.</p> <p>Take a Normal Distribution, and model it as \(\mathcal{N}(0, \sigma^2)\). What if I keep sampling an element from this distribution, every second, for an hour?</p> <p>As you can see, we get random noise ‚Äì not very interesting. Note, of course, that if we take these samples and put them in a histogram, we shall regain the normal distribution, given a good number of samples drawn. That is the consequence of the Monte Carlo sampling process. Again, not very interesting.</p> <p>What is interesting, however, is if we tweak the equation a little, and add a dependency of the normal distribution on the past sample ‚Äì we go from \(\mathcal{N}(0, \sigma^2)\) to \(\mathcal{N}(\theta, \sigma^2)\), where \(\theta\) is the last sample drawn, in each case.</p> <p>We shall now see a drastically different result ‚Äì</p> <p>Note here that these samples will <em>not</em> form a histogram-representation of the normal distribution.</p> <p>What is happening here? How did we go from the undecipherably, decidedly noisy output to a rather <em>pretty</em> line, with just a small dependence? Well, that is the beauty of the first order Markov Process ‚Äì where each element is dependent on (and <em>only</em> dependent on) the last sample from the distribution.</p> <p>Think two-gram language models, and think Brownian motion. With a simple change of variables, we go from modelling almost nothing interesting, to how pollen particles behave on the surface of non-viscous fluids, how stock markets function, and how (eventually), we can model any function with a Gaussian process.</p> <h2 id="distributing-functions-instead-of-variables">Distributing functions instead of variables.</h2> <p>With a different seed, I could have generate a very different path for my proposed brownian distribution, without really changing any of my parameters. Given that function, I propose the following: <em>each path traced by a particle exhibiting Brownian motion is the realization of a probability distribution over a set of functions</em>.</p> <p>It is somewhat intuitive that this can be considered true. You collapse the probability at every timestep \(t\) into a value, \(y\), until you have a realization at every infinitesimal point of the axis, and then you plot those realizations.</p> <p>This idea brings us comfortably into the domain of Gaussian Processes.</p> <p>Imagine this: each 2D function (\(y\) is a function of \(x\)), is, ultimately, formed when we find a value of \(y\) corresponding with \(x\). What if, instead of knowing \(y\) fully at some point, I produced some <em>confidence</em> metric to quantify how sure I am of \(y\) being \(y\)?</p> <p>If that confidence was modelled by (as things are in Bayesian statistics) a Gaussian distribution, we could reasonably have a good visualization by simply plotting a candlestick with the width of the standard deviation, centered at the mean.</p> <p>The mean, of course, would just be the point that avoids discontinuity in the proposed \(f(x)\) distribution.</p> <p>No big deal, right? Let‚Äôs expand this idea.</p> <p>What if, instead of just <em>one</em> point, this expanded to <em>every</em> point in the space, each with a mean and a standard deviation?</p> <p>Well, then, assuming the probability of collapsing each of these ‚Äúwavefunctions‚Äù into their most likely values is independent, we could trace a line through the <em>means</em> (which is to say, expectations) of all of these infinite Gaussian distributions and call it a day (and a function!).</p> <p>For good measure, we‚Äôll even plot an area that traps every other function within one standard deviation of that most likely function.</p> <p>Note that instead of a bunch of different Gaussian distributions, we can consider this function as the realization of one infinite-dimensional Gaussian distribution (in mathematics, when something is parameterized by infinite parameters, we just shift to calling it <em>non-parameterized</em>. Why? Because mathematicians are mercurial).</p> <p>These ‚Äúinfinite-dimensional (and hence, not parameterized) Gaussian distributions‚Äù aren‚Äôt truly Gaussian distributions <em>per se</em>, since at infinite means and infinite variances and covariances, one can‚Äôt really verify the properties that a Gaussian distributions is deemed to have.</p> <p>We <em>can</em>, however, say this:</p> <blockquote> <p>A Gaussian process is a system of infinite distributions, all <em>finite subsets</em> of which will always be in Gaussian distribution with a defined mean and a covariance matrix.</p> </blockquote> <p>Wonderful! Now, we have the theoretical understanding for what a Gaussian process <em>is</em>. What we still don‚Äôt know is how to <em>fit</em> it to some data.</p> <h2 id="putting-the-gaussian-in-the-gaussian-process">Putting the Gaussian in the Gaussian Process.</h2> <p>Alright, so you have a way to quantify functions as probability distributions. Big deal ‚Äì it‚Äôs not really useful unless you understand how to make predictions off of it, right?</p> <p>Turns out there‚Äôs a way to do that: and it‚Äôs a closed form solution! But to get to that, we must first understand a few things.</p> <h3 id="the-covariance-matrix-is-the-prior">The Covariance matrix is the prior.</h3> <p>As with all priors, to make a prediction, we first need to pack all of our assumptions about the underlying ground-truth into some quantity. In the case of Gaussian distributions, that quantity is encoded in how we think each of the infinite parameters in the Gaussian distribution is correlated.</p> <p>The <em>covariance matrix</em>, \(k(x_a, x_b)\) models the joint variability of the points in the Gaussian process, so it tells us how much each of \(x_a\) and \(x_b\) varies with respect to the other.</p> <p>Now, the covariance matrix needs to have certain properties, but let us try to approach them intuitively. The variance, in one dimension, is the square of the standard deviation ‚Äì in other words, you must be able to take the square-root of the variance to get the standard deviation.</p> <p>In N-dimensional matrices, what passes as the square-root is found through the <strong>Cholesky Decomposition</strong>, where we break an arbitrary matrix, \(\mathcal{N}\) into a product, \(M^T \cdot M\), in some ways the equivalent of its square-roots.</p> <p>We can prove that a Cholesky Decomposition is only possible if the matrix is <strong>positive semi-definite</strong>, which is to say that it is a symmetric matrix with all eigenvalues non-negative (the semi-definite because definite when the eigenvalues are strictly positive). Note that some texts here mention that the matrix needs to actually be positive-<em>definite</em>, instead of allowing semi-definite matrices to suffice.</p> <p>From hereon, we will use the terms <strong>covariance function</strong> and <strong>kernel function</strong> interchangeably.</p> <p>The kernel function is a prior applied on the process that establishes a certain relationship between each pair of variables involved. Note that this is <em>not</em> a matrix, since the number of pairs is infinite, so the function is a valuable abstraction.</p> <p>We use the <strong>Radial Basis Function</strong>, or the exponentiated quadratic covariance function, which is our go-to, vanilla kernel when we don‚Äôt know much about our function. We could also have a <strong>periodic kernel</strong>, which predicts that current trends will recur in our function, and so on. A <strong>linear kernel</strong> imposes that we have a straight line instead of a curve, and therefore makes our prediction boil down to simple Bayesian Linear Regression.</p> <p>The RBF causes our functions to come out as a <strong>locally smooth</strong> with high probability. Nearby function values are highly correlated, and the correlation drops off as we move farther apart in the input space.</p> <h3 id="the-prior-visualized">The prior, visualized.</h3> <p>Let‚Äôs take a step back and see what we have till now ‚Äì we are imagining a vector-space of random functions, and each function is described as an infinite number of points, each of which is described by its own mean, and an overall covariance matrix.</p> <p>The function at some \(x_0\) is <em>most</em> likely to be at the mean, \(\mu_0\) at that \(x_0\), but actually could be anywhere in that vertical line, \(x = x_0\). We can, however, restrict it to some standard deviation, so we can have a reasonable estimate of the value of \(f(x)\) at \(x_0\).</p> <p>The covariance helps us to see how collapsing the probability at that point helps in determining all subsequent points.</p> <p>This ‚Äúperfect‚Äù abstraction of a function with infinite parameters we can plot can be realized by ‚Äúdemoting‚Äù it to a subset of \(n\) Gaussian distributions, a marginal distribution expressed as, \(y \sim \mathcal{N}(\mu, \Sigma)\) with a mean vector \(\mu(X)\), and a covariance \(\Sigma = k(X,X)\), where \(k(x_a, x_b)\) was our kernel function.</p> <p>Let us say that our \(\mu\) is \(0\). This implies that the \(\mu\) vector is a row of zeroes of infinite length.</p> <p>Now, we can draw correlated samples from this \(n\) dimensional Gaussian, \(\mathcal{N}(0, k(X,X))\). For now, we can approximate \(4\) random functions with a \(100\) dimensional Gaussian distribution.</p> <p>The RBF Kernel correlates nearby points much more than points further away. As a result, in our \(100\) dimensional example, points corresponding to \(x_{50}\) and \(x_{51}\) could have a covariance of, say, \(0.95\), while that for \(x_{50}\) and \(x_{99}\) might have a covariance of just \(0.05\).</p> <h2 id="building-a-regression-model">Building a Regression Model.</h2> <p>We start by noting that <em>all</em> functions that <em>can</em> exist in some \(n\) dimensional space have already been encoded into our simple, zero mean, RBF kernel Gaussian process. Based on the data, they are just more or less likely to be realized.</p> <p>What we ought to do is to take the data, and use the information to push the prior towards a posterior that maximizes the likelihood of realizing the ground-truth function from our Gaussian Process.</p> <p>Let‚Äôs keep this simple: the professor has given you some data, you have your very pretty kernel function as your prior (and from there, a prior distribution over the function space in the form of a zero-mean Gaussian Process, \(h(\cdot) \sim \mathcal{GP}(0, k(x_a, x_b))\)), and now he‚Äôs told you to predict what‚Äôs going to be the function‚Äôs values in a bunch of places.</p> <p>The data you get from the professor is of size \(n_1\), expressed as (\(X_1, y_1\)); you have your prior kernel function, the RBF; and you have a set of \(n_2\) points, expressed as \(X_2\) ‚Äì the places where he‚Äôs asked you to predict the function‚Äôs values.</p> <p>Ergo, all you have to do is guess \(y_2 = f(X_2)\) as well as possible.</p> <p>Let us look at what operations we can do here. We have a function, and some data. The function tells us how we think the data is correlated, so let‚Äôs find evaluate the function on all of the data. Considering \(\Sigma_{ab} = k(X_a, X_b)\), there are four things we <em>can</em> evaluate ‚Äì \(\Sigma_{11}\), \(\Sigma_{22}\), \(\Sigma_{12}\), and \(\Sigma_{21}\).</p> <p>Again, we are to predict \(y_2\), and we have at hand, \(y_1\), \(X_1\), and \(X_2\).</p> <p>Among them, \(X_2\) doesn‚Äôt really encode any new information, it just instructs us about where to evaluate the data.</p> <p>As always, we now use Bayes‚Äô Rule to find \(p(y_2 \vert y_1, X_1, X_2)\), that is, we try to find a posterior for \(y_2\) given all of the data we have.</p> <p>Gaussian distributions have certain properties that makes calculating this <em>much</em> simpler. As a recap (or as a hint), make sure that you know the following identities:</p> <p>Given \(p(x_1) = \mathcal{N}(\mu_1, \Sigma_{11})\), and \(p(x_2) = \mathcal{N}(\mu_2, \Sigma_{22})\), it follows that,</p> \[p(x_1 | x_2) = \mathcal{N}(\mu_{x_1|x_2}, \Sigma_{x_1 | x_2})\] \[\mu_{1|2} = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2)\] \[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{12}^T\] <p>Let us consider \(\mu_2\) and \(\mu_1\)to be 0 in this case. Then, our posterior becomes:</p> <p>\(\mu_{2 \vert 1} = \Sigma_{21} \Sigma_{11}^{-1} {x_1}\) and our posterior covariance, \(\Sigma_{2 \vert 1} = \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{21}^T\)</p> <p>Note: in our identities, we considered dataset \(1\) conditioned on \(2\), and here we‚Äôre doing \(2\) conditioned on \(1\), so the subscripts have changed. It is a good exercise to go through both and see what depends on what. I highly recommend a readthrough of the Conditional Probability section of Peter Roelants‚Äôs <a href="https://peterroelants.github.io/posts/multivariate-normal-primer/#Conditional-distribution" target="_blank" rel="noopener noreferrer">Multivariate Normal Primer</a>.</p> <p>Also note that \(\Sigma_{11}\) is symmetrical, and therefore equal to its transpose.</p> <p>And voila! We have our Gaussian Process regression model ready to go!</p> <h2 id="towards-faster-regression">Towards faster regression.</h2> <p>Now that we know our regression works, let us see how much data we need to get to a good approximation of the ground-truth function ‚Äì after all, Bayesian statistics is about confidences first, and we should know how confident we are of the true function before we proceed.</p> <p>We first try to approximate a simple trigonometric function, \(f(x) = sin(x) + cos(x)\) over a support of \(- 3\) to \(3\), with \(10,000\) randomly-chosen data-points. We expect a very confident answer, and we do get one ‚Äì albeit after quite a while, due to the time complexity, which is something we do not want.</p> <p>What happens when we reduce this number to, say, \(5\) randomly-chosen data-points? Also, what happens at \(10\)?</p> <p>In fact, let us use our own Gaussian Process to predict how the complexity of our algorithm grows with time, inspired by Zhenwen Dai‚Äôs session on Sparse Gaussian Process in GPSS 2019 ‚Äì with the runtime for \(N = {5, 10, 50, 100, 500, 1000}\).</p> <p>It is quite clear that our confidence has decreased rather drastically: something we also do not want. This warrants the question ‚Äì one, is there a tradeoff between number of data-points and the time required for inference that we are willing to take, and two (and more pertinent to the problem at hand), if we take (instead of randomly choosing) a heuristic method of manipulating our matrix, or our data-points in such a way that we <em>choose</em> the most important data-points, how far could we go?</p> <h2 id="a-short-discussion-on-the-time-complexity-of-the-enterprise">A short discussion on the Time Complexity of the enterprise.</h2> <p>Notice that the matrix \(\Sigma_{xy} \in \mathbb{R}^{N \times N}\) needs to be inverted and taken the determinant of for analysis and inference ‚Äì which incurs a cost of \(\mathcal{O}(N^3)\) in time and \(\mathcal{O}(N^2)\) in memory.</p> <p>That is the primary problem with Gaussian Processes ‚Äì \(\mathcal{O}(N^3)\) in time is more than we can afford.</p> <p>To preserve the accuracy of our Gaussian Process, what we must do is shed away all redundant data, while retaining the information that collapses the probabilities of our Gaussian Process at novel locations. Remember that in a matrix, the rank is the number of linearly independent equations that we can glean out of it ‚Äì a full-rank matrix would be one where <em>nothing</em> is wasted, and a low-rank matrix would be one which is wasteful and can be compressed (and so we shall do just that).</p> <p>Our aim, now, is to determine whether (and it almost always is) our data matrix is low-rank, and then transform it to a matrix that encodes more novelty into our prediction, and then use <em>that</em> matrix for inversion.</p> <p>Simple.</p> <h3 id="the-nystr√∂m-approximation-efficient-use-of-random-samples">The Nystr√∂m approximation: efficient use of random samples.</h3> <p>For a Gaussian Process, the log-likelihood is \(\log p(y \vert X) = \log \mathcal{N} (y \vert 0, K(X_1,X_1))\), where \(X_1\) is, as discussed, the points where the data is known. \(K\) is the covariance matrix over the data, according to our kernel function.</p> <p>Our assumption will be that \(K\) is low-rank, and hence wasteful. Our goal is to optimize K for eventual inference.</p> <p>What follows is a series of matrix computations, so we shall keep the following identities in mind. For two matrices, if you have a matrix \(K\) of size \(N \times M\), and you truncate that matrix with a subset of values, to size \(M \times M\), to \(S\), keep in mind that \(K S^{-1} K^T\) retains the size \(N \times N\).</p> <p>This, in effect, ‚Äúsparsifies‚Äù our matrix, thereby reducing the amount of raw computations required to calculate the inverse of \(\Sigma\).</p> <p>The Nystr√∂m approximation, therefore, requires us to,</p> <ul> <li>Randomly pick a subset from the data, say \(D \in \mathbb{R}^{M \times 1}\).</li> <li>Use our kernel function to find \(S\), where \(S = K(D, D)\).</li> <li>Use the formula that we discussed to get a covariance matrix of size \(N \times N\) back after the computation \(K S^{-1} K^T\), where \(S \in \mathbb{R}^{M \times M}\).</li> <li>We set \(\mathcal{K} = K S^{-1} K^T\), and use that as our covariance matrix.</li> </ul> <p>Again, the log likelihood, as discussed, is given with our new covariance matrix and some added noise, as \(\mathcal{L} = \log p(y \vert X, \theta) = \log \mathcal{N} (y \vert 0, \mathcal{K} + \sigma^2 I)\).</p> <p>Simply expanding \(\mathcal{L}\) yields \(- \frac{1}{2} [ \log \vert 2 \pi (\mathcal{K} + \sigma^2 I) \vert + y^T (\mathcal{K} + \sigma^2I)^{-1} y ]\), which does not lead to any speedup in computation ‚Äì we still have to perform \(N^3\) calculations to invert \(\Sigma\).</p> <p>However, we note that \((A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + V A^{-1}U)^{-1}VA^{-1}\), where A, U, C and V have the dimensions necessary for said operations. This is called the <em>Woodbury matrix identity</em>, and essentially says that the inverse of a rank-\(k\) correction to a matrix can be computed by doing a rank-\(k\) correction to the inverse of the original matrix.</p> <h3 id="aside-on-rank-corrections">Aside: on rank corrections.</h3> <p>While working on the Woodbury formula, we talked about <em>rank corrections</em> ‚Äì what is that supposed to mean?</p> <p>Consider a linear regression problem, when you‚Äôre trying to compute the coefficients of a linear model. The closed form solution to a plain linear regression model is, \((X^T X)^{-1} X^T Y\). Assume that this solution has full-column rank, which means that the variables involved are linearly independent.</p> <p>What are we to do when we observe new samples, \(x_{n+1}, x_{n+2}, ... \in \mathbb{R}^k\)? Rather than explicitly re-computing the matrix inverse of \(M = (X^T X)\) wastefully, we use the matrix inversion lemma to update the value of this \(M\), which we should store somewhere when saving our model.</p> <h3 id="back-to-the-nystr√∂m-approximation">Back to the Nystr√∂m Approximation</h3> <p>We note here that the Woodbury formula reduces the term \((\mathcal{K} + \sigma^2 I)^{-1}\), which appears in the expansion for our Normal distribution, to \([\sigma^{-2}I - \sigma^{-4}K(S + \sigma^{-2}K^T K )^{-1}K^T]\).</p> <p>Note that here, \(K \in \mathbb{R}^{N \times M}\) and \(S \in \mathbb{R}^{M \times M}\).</p> <p>Consequently, the matrix inverted, \((S + \sigma^{-2}K^T K )\) is of size \(M \times M\).</p> <p>Immediately, our complexity falls to \(\mathcal{O}(NM^2)\). That is the Nystr√∂m Approximation, and our approximation gets better and better the closer we get to the whole dataset. The subset selection is generally done randomly.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2024 Progyan Das. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <d-bibliography src="/assets/bibliography/2022-09-25-gp.bib"></d-bibliography> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>