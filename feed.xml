<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://plugyawn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://plugyawn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-07T03:14:40+00:00</updated><id>https://plugyawn.github.io/feed.xml</id><title type="html">This is Progyan Das</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On taking back power.</title><link href="https://plugyawn.github.io/blog/2023/metaiitgn/" rel="alternate" type="text/html" title="On taking back power."/><published>2023-12-23T00:00:00+00:00</published><updated>2023-12-23T00:00:00+00:00</updated><id>https://plugyawn.github.io/blog/2023/metaiitgn</id><content type="html" xml:base="https://plugyawn.github.io/blog/2023/metaiitgn/"><![CDATA[<blockquote> <p>The history of all hitherto existing society is the history of class struggles.</p> </blockquote> <p>IIT Gandhinagar is a place of unexpressed liminality. An old institution in its infancy, bricks and mortar given a destiny before they were masoned together. It is, by no exaggeration, a special place, a place of juxtapositions, golden waiting to become gold.</p> <p>And of course, she buckles under the weight of expectation. It is unnatural not to, at her age – her walls tremble from the weight of stray paint these days, her councils are brought down by substance, her students know not when they can claim the name of the institution they believed they would come into.</p> <p>It is visible in the pretty ivy that we seeded and yet pretend comes from neglect, in the whitewashed architecture that pretends to be postmodern but shies away into brutalism, in the gates that we build and yet refuse to open. An institute built from the same smoke and fire that it demonizes today.</p> <p>IIT Gandhinagar is younger than those who inhabit it, and until the turn of this decade, her constitution, her principles were half as old as those who inherited them. Principles writ by the same people who lived them. Power pooled into the hands of the minority is the antithesis, the death, of democracy. A plutarchy that replaces wealth with unfounded experience is hardly different from a failing oligarchy – and it is dangerous for them to decide which side of the grass is greener, for it means that we are led by the blind into the dark.</p> <h2 id="why-metaiitgandhinagar">Why \(MetaIITGandhinagar\)?</h2> <p>\(MetaIITGandhinagar\) (short: \(Meta\), pronounced “My Gee-En”, if you care for that) is about taking back what is ours – the means to bring change. Vikrant Verma, the founder of \(MetaKGP\), which inspired in us the idea for this platform, and to which we pay homage to through our name, laid emphasis on the predilection of smart students for avoiding drudgery. That’s especially true, with the entire student community the size of individual programmes at IIT Kharagpur. That is also why we have historically relied on administrative machinery to get work done – a crutch that has predictably come to haunt us today. It is high time we take a pause, a deep breath, and reevaluate, reassess, and see why and how we can become self-dependent, as a student body.</p> <p>It starts with three pillars – <strong>communication</strong>, <strong>information</strong>, and <strong><em>power</em></strong>.</p> <p><strong>Communication</strong> means that we have a way to disseminate information that isn’t controlled and monitored by the administration, that reaches across the student population. Emails that are sent across to the student body need to be moderated, of course – society descends into savagery otherwise, or becomes one big Instagram crowd. Both are undesirable. Moderation is necessary, but moderation must come from the students themselves, not from a disconnected entity in an office on a power-trip. That is why <code class="language-plaintext highlighter-rouge">chat.metaiitgn.org</code> exists – with both students and professors on the same distributed chat server, moderated by both, it is a way to maintain balance.</p> <p><strong>Information</strong> means that history is recorded. That problems in the past do not become ghosts of the problems in the present. That students do not become dissuaded by false reason and needless misinformation, spread by bad actors. <code class="language-plaintext highlighter-rouge">wiki.metaiitgn.org</code> aims to become a community-driven wellspring of information for everyone in the community – so people take decisions, and <em>communicate</em>, with the right priors.</p> <p>The problem of <strong>Power</strong> is harder, and is solved through time. And for that, we invite <em>you</em> to \(Meta\), if you want to become part of the solution. A number of important people were involved in the construction of this idea, and we hope that it becomes the foundation of something that IIT Gandhinagar wanted to be, in its infancy. In admittedly dramatic terms, it is the battle for the soul of this institution. In less dramatic terms, it is a way for you to have a legacy. To leave something behind. To become part of a society of engineers and scientists who saw the gaping leak in our roof, and decided to fix it, because that is what we do.</p> <p>God knows it has rained for long enough.</p>]]></content><author><name>Progyan Das</name></author><summary type="html"><![CDATA[Reflections on MetaKGP, MetaIITGN and the future of IIT Gandhinagar.]]></summary></entry><entry><title type="html">Understanding Gaussian Processes, from scratch.</title><link href="https://plugyawn.github.io/blog/2022/gp/" rel="alternate" type="text/html" title="Understanding Gaussian Processes, from scratch."/><published>2022-09-25T00:00:00+00:00</published><updated>2022-09-25T00:00:00+00:00</updated><id>https://plugyawn.github.io/blog/2022/gp</id><content type="html" xml:base="https://plugyawn.github.io/blog/2022/gp/"><![CDATA[<h2 id="brownian-motion">Brownian Motion</h2> <p>As with most of Bayesian theory, we start with Bayes’ Rule,</p> \[P(\theta|y) = \frac{P(y|\theta) \cdot P(\theta)}{P(y)}\] <p>Until now, we have taken \(P(x)\) to mean the probability that \(x\) takes on a particular <em>value</em>, but now the game has changed – we shall now take \(P(x)\) to mean the probability that we encounter a particular <strong>function</strong>.</p> <p>Indeed, this simple idea is at the heart of what we call the Gaussian Process.</p> <p>Take a Normal Distribution, and model it as \(\mathcal{N}(0, \sigma^2)\). What if I keep sampling an element from this distribution, every second, for an hour?</p> <p>As you can see, we get random noise – not very interesting. Note, of course, that if we take these samples and put them in a histogram, we shall regain the normal distribution, given a good number of samples drawn. That is the consequence of the Monte Carlo sampling process. Again, not very interesting.</p> <p>What is interesting, however, is if we tweak the equation a little, and add a dependency of the normal distribution on the past sample – we go from \(\mathcal{N}(0, \sigma^2)\) to \(\mathcal{N}(\theta, \sigma^2)\), where \(\theta\) is the last sample drawn, in each case.</p> <p>We shall now see a drastically different result –</p> <p>Note here that these samples will <em>not</em> form a histogram-representation of the normal distribution.</p> <p>What is happening here? How did we go from the undecipherably, decidedly noisy output to a rather <em>pretty</em> line, with just a small dependence? Well, that is the beauty of the first order Markov Process – where each element is dependent on (and <em>only</em> dependent on) the last sample from the distribution.</p> <p>Think two-gram language models, and think Brownian motion. With a simple change of variables, we go from modelling almost nothing interesting, to how pollen particles behave on the surface of non-viscous fluids, how stock markets function, and how (eventually), we can model any function with a Gaussian process.</p> <h2 id="distributing-functions-instead-of-variables">Distributing functions instead of variables.</h2> <p>With a different seed, I could have generate a very different path for my proposed brownian distribution, without really changing any of my parameters. Given that function, I propose the following: <em>each path traced by a particle exhibiting Brownian motion is the realization of a probability distribution over a set of functions</em>.</p> <p>It is somewhat intuitive that this can be considered true. You collapse the probability at every timestep \(t\) into a value, \(y\), until you have a realization at every infinitesimal point of the axis, and then you plot those realizations.</p> <p>This idea brings us comfortably into the domain of Gaussian Processes.</p> <p>Imagine this: each 2D function (\(y\) is a function of \(x\)), is, ultimately, formed when we find a value of \(y\) corresponding with \(x\). What if, instead of knowing \(y\) fully at some point, I produced some <em>confidence</em> metric to quantify how sure I am of \(y\) being \(y\)?</p> <p>If that confidence was modelled by (as things are in Bayesian statistics) a Gaussian distribution, we could reasonably have a good visualization by simply plotting a candlestick with the width of the standard deviation, centered at the mean.</p> <p>The mean, of course, would just be the point that avoids discontinuity in the proposed \(f(x)\) distribution.</p> <p>No big deal, right? Let’s expand this idea.</p> <p>What if, instead of just <em>one</em> point, this expanded to <em>every</em> point in the space, each with a mean and a standard deviation?</p> <p>Well, then, assuming the probability of collapsing each of these “wavefunctions” into their most likely values is independent, we could trace a line through the <em>means</em> (which is to say, expectations) of all of these infinite Gaussian distributions and call it a day (and a function!).</p> <p>For good measure, we’ll even plot an area that traps every other function within one standard deviation of that most likely function.</p> <p>Note that instead of a bunch of different Gaussian distributions, we can consider this function as the realization of one infinite-dimensional Gaussian distribution (in mathematics, when something is parameterized by infinite parameters, we just shift to calling it <em>non-parameterized</em>. Why? Because mathematicians are mercurial).</p> <p>These “infinite-dimensional (and hence, not parameterized) Gaussian distributions” aren’t truly Gaussian distributions <em>per se</em>, since at infinite means and infinite variances and covariances, one can’t really verify the properties that a Gaussian distributions is deemed to have.</p> <p>We <em>can</em>, however, say this:</p> <blockquote> <p>A Gaussian process is a system of infinite distributions, all <em>finite subsets</em> of which will always be in Gaussian distribution with a defined mean and a covariance matrix.</p> </blockquote> <p>Wonderful! Now, we have the theoretical understanding for what a Gaussian process <em>is</em>. What we still don’t know is how to <em>fit</em> it to some data.</p> <h2 id="putting-the-gaussian-in-the-gaussian-process">Putting the Gaussian in the Gaussian Process.</h2> <p>Alright, so you have a way to quantify functions as probability distributions. Big deal – it’s not really useful unless you understand how to make predictions off of it, right?</p> <p>Turns out there’s a way to do that: and it’s a closed form solution! But to get to that, we must first understand a few things.</p> <h3 id="the-covariance-matrix-is-the-prior">The Covariance matrix is the prior.</h3> <p>As with all priors, to make a prediction, we first need to pack all of our assumptions about the underlying ground-truth into some quantity. In the case of Gaussian distributions, that quantity is encoded in how we think each of the infinite parameters in the Gaussian distribution is correlated.</p> <p>The <em>covariance matrix</em>, \(k(x_a, x_b)\) models the joint variability of the points in the Gaussian process, so it tells us how much each of \(x_a\) and \(x_b\) varies with respect to the other.</p> <p>Now, the covariance matrix needs to have certain properties, but let us try to approach them intuitively. The variance, in one dimension, is the square of the standard deviation – in other words, you must be able to take the square-root of the variance to get the standard deviation.</p> <p>In N-dimensional matrices, what passes as the square-root is found through the <strong>Cholesky Decomposition</strong>, where we break an arbitrary matrix, \(\mathcal{N}\) into a product, \(M^T \cdot M\), in some ways the equivalent of its square-roots.</p> <p>We can prove that a Cholesky Decomposition is only possible if the matrix is <strong>positive semi-definite</strong>, which is to say that it is a symmetric matrix with all eigenvalues non-negative (the semi-definite because definite when the eigenvalues are strictly positive). Note that some texts here mention that the matrix needs to actually be positive-<em>definite</em>, instead of allowing semi-definite matrices to suffice.</p> <p>From hereon, we will use the terms <strong>covariance function</strong> and <strong>kernel function</strong> interchangeably.</p> <p>The kernel function is a prior applied on the process that establishes a certain relationship between each pair of variables involved. Note that this is <em>not</em> a matrix, since the number of pairs is infinite, so the function is a valuable abstraction.</p> <p>We use the <strong>Radial Basis Function</strong>, or the exponentiated quadratic covariance function, which is our go-to, vanilla kernel when we don’t know much about our function. We could also have a <strong>periodic kernel</strong>, which predicts that current trends will recur in our function, and so on. A <strong>linear kernel</strong> imposes that we have a straight line instead of a curve, and therefore makes our prediction boil down to simple Bayesian Linear Regression.</p> <p>The RBF causes our functions to come out as a <strong>locally smooth</strong> with high probability. Nearby function values are highly correlated, and the correlation drops off as we move farther apart in the input space.</p> <h3 id="the-prior-visualized">The prior, visualized.</h3> <p>Let’s take a step back and see what we have till now – we are imagining a vector-space of random functions, and each function is described as an infinite number of points, each of which is described by its own mean, and an overall covariance matrix.</p> <p>The function at some \(x_0\) is <em>most</em> likely to be at the mean, \(\mu_0\) at that \(x_0\), but actually could be anywhere in that vertical line, \(x = x_0\). We can, however, restrict it to some standard deviation, so we can have a reasonable estimate of the value of \(f(x)\) at \(x_0\).</p> <p>The covariance helps us to see how collapsing the probability at that point helps in determining all subsequent points.</p> <p>This “perfect” abstraction of a function with infinite parameters we can plot can be realized by “demoting” it to a subset of \(n\) Gaussian distributions, a marginal distribution expressed as, \(y \sim \mathcal{N}(\mu, \Sigma)\) with a mean vector \(\mu(X)\), and a covariance \(\Sigma = k(X,X)\), where \(k(x_a, x_b)\) was our kernel function.</p> <p>Let us say that our \(\mu\) is \(0\). This implies that the \(\mu\) vector is a row of zeroes of infinite length.</p> <p>Now, we can draw correlated samples from this \(n\) dimensional Gaussian, \(\mathcal{N}(0, k(X,X))\). For now, we can approximate \(4\) random functions with a \(100\) dimensional Gaussian distribution.</p> <p>The RBF Kernel correlates nearby points much more than points further away. As a result, in our \(100\) dimensional example, points corresponding to \(x_{50}\) and \(x_{51}\) could have a covariance of, say, \(0.95\), while that for \(x_{50}\) and \(x_{99}\) might have a covariance of just \(0.05\).</p> <h2 id="building-a-regression-model">Building a Regression Model.</h2> <p>We start by noting that <em>all</em> functions that <em>can</em> exist in some \(n\) dimensional space have already been encoded into our simple, zero mean, RBF kernel Gaussian process. Based on the data, they are just more or less likely to be realized.</p> <p>What we ought to do is to take the data, and use the information to push the prior towards a posterior that maximizes the likelihood of realizing the ground-truth function from our Gaussian Process.</p> <p>Let’s keep this simple: the professor has given you some data, you have your very pretty kernel function as your prior (and from there, a prior distribution over the function space in the form of a zero-mean Gaussian Process, \(h(\cdot) \sim \mathcal{GP}(0, k(x_a, x_b))\)), and now he’s told you to predict what’s going to be the function’s values in a bunch of places.</p> <p>The data you get from the professor is of size \(n_1\), expressed as (\(X_1, y_1\)); you have your prior kernel function, the RBF; and you have a set of \(n_2\) points, expressed as \(X_2\) – the places where he’s asked you to predict the function’s values.</p> <p>Ergo, all you have to do is guess \(y_2 = f(X_2)\) as well as possible.</p> <p>Let us look at what operations we can do here. We have a function, and some data. The function tells us how we think the data is correlated, so let’s find evaluate the function on all of the data. Considering \(\Sigma_{ab} = k(X_a, X_b)\), there are four things we <em>can</em> evaluate – \(\Sigma_{11}\), \(\Sigma_{22}\), \(\Sigma_{12}\), and \(\Sigma_{21}\).</p> <p>Again, we are to predict \(y_2\), and we have at hand, \(y_1\), \(X_1\), and \(X_2\).</p> <p>Among them, \(X_2\) doesn’t really encode any new information, it just instructs us about where to evaluate the data.</p> <p>As always, we now use Bayes’ Rule to find \(p(y_2 \vert y_1, X_1, X_2)\), that is, we try to find a posterior for \(y_2\) given all of the data we have.</p> <p>Gaussian distributions have certain properties that makes calculating this <em>much</em> simpler. As a recap (or as a hint), make sure that you know the following identities:</p> <p>Given \(p(x_1) = \mathcal{N}(\mu_1, \Sigma_{11})\), and \(p(x_2) = \mathcal{N}(\mu_2, \Sigma_{22})\), it follows that,</p> \[p(x_1 | x_2) = \mathcal{N}(\mu_{x_1|x_2}, \Sigma_{x_1 | x_2})\] \[\mu_{1|2} = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} (x_2 - \mu_2)\] \[\Sigma_{1|2} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{12}^T\] <p>Let us consider \(\mu_2\) and \(\mu_1\)to be 0 in this case. Then, our posterior becomes:</p> <p>\(\mu_{2 \vert 1} = \Sigma_{21} \Sigma_{11}^{-1} {x_1}\) and our posterior covariance, \(\Sigma_{2 \vert 1} = \Sigma_{22} - \Sigma_{21} \Sigma_{11}^{-1} \Sigma_{21}^T\)</p> <p>Note: in our identities, we considered dataset \(1\) conditioned on \(2\), and here we’re doing \(2\) conditioned on \(1\), so the subscripts have changed. It is a good exercise to go through both and see what depends on what. I highly recommend a readthrough of the Conditional Probability section of Peter Roelants’s <a href="https://peterroelants.github.io/posts/multivariate-normal-primer/#Conditional-distribution">Multivariate Normal Primer</a>.</p> <p>Also note that \(\Sigma_{11}\) is symmetrical, and therefore equal to its transpose.</p> <p>And voila! We have our Gaussian Process regression model ready to go!</p> <h2 id="towards-faster-regression">Towards faster regression.</h2> <p>Now that we know our regression works, let us see how much data we need to get to a good approximation of the ground-truth function – after all, Bayesian statistics is about confidences first, and we should know how confident we are of the true function before we proceed.</p> <p>We first try to approximate a simple trigonometric function, \(f(x) = sin(x) + cos(x)\) over a support of \(- 3\) to \(3\), with \(10,000\) randomly-chosen data-points. We expect a very confident answer, and we do get one – albeit after quite a while, due to the time complexity, which is something we do not want.</p> <p>What happens when we reduce this number to, say, \(5\) randomly-chosen data-points? Also, what happens at \(10\)?</p> <p>In fact, let us use our own Gaussian Process to predict how the complexity of our algorithm grows with time, inspired by Zhenwen Dai’s session on Sparse Gaussian Process in GPSS 2019 – with the runtime for \(N = {5, 10, 50, 100, 500, 1000}\).</p> <p>It is quite clear that our confidence has decreased rather drastically: something we also do not want. This warrants the question – one, is there a tradeoff between number of data-points and the time required for inference that we are willing to take, and two (and more pertinent to the problem at hand), if we take (instead of randomly choosing) a heuristic method of manipulating our matrix, or our data-points in such a way that we <em>choose</em> the most important data-points, how far could we go?</p> <h2 id="a-short-discussion-on-the-time-complexity-of-the-enterprise">A short discussion on the Time Complexity of the enterprise.</h2> <p>Notice that the matrix \(\Sigma_{xy} \in \mathbb{R}^{N \times N}\) needs to be inverted and taken the determinant of for analysis and inference – which incurs a cost of \(\mathcal{O}(N^3)\) in time and \(\mathcal{O}(N^2)\) in memory.</p> <p>That is the primary problem with Gaussian Processes – \(\mathcal{O}(N^3)\) in time is more than we can afford.</p> <p>To preserve the accuracy of our Gaussian Process, what we must do is shed away all redundant data, while retaining the information that collapses the probabilities of our Gaussian Process at novel locations. Remember that in a matrix, the rank is the number of linearly independent equations that we can glean out of it – a full-rank matrix would be one where <em>nothing</em> is wasted, and a low-rank matrix would be one which is wasteful and can be compressed (and so we shall do just that).</p> <p>Our aim, now, is to determine whether (and it almost always is) our data matrix is low-rank, and then transform it to a matrix that encodes more novelty into our prediction, and then use <em>that</em> matrix for inversion.</p> <p>Simple.</p> <h3 id="the-nyström-approximation-efficient-use-of-random-samples">The Nyström approximation: efficient use of random samples.</h3> <p>For a Gaussian Process, the log-likelihood is \(\log p(y \vert X) = \log \mathcal{N} (y \vert 0, K(X_1,X_1))\), where \(X_1\) is, as discussed, the points where the data is known. \(K\) is the covariance matrix over the data, according to our kernel function.</p> <p>Our assumption will be that \(K\) is low-rank, and hence wasteful. Our goal is to optimize K for eventual inference.</p> <p>What follows is a series of matrix computations, so we shall keep the following identities in mind. For two matrices, if you have a matrix \(K\) of size \(N \times M\), and you truncate that matrix with a subset of values, to size \(M \times M\), to \(S\), keep in mind that \(K S^{-1} K^T\) retains the size \(N \times N\).</p> <p>This, in effect, “sparsifies” our matrix, thereby reducing the amount of raw computations required to calculate the inverse of \(\Sigma\).</p> <p>The Nyström approximation, therefore, requires us to,</p> <ul> <li>Randomly pick a subset from the data, say \(D \in \mathbb{R}^{M \times 1}\).</li> <li>Use our kernel function to find \(S\), where \(S = K(D, D)\).</li> <li>Use the formula that we discussed to get a covariance matrix of size \(N \times N\) back after the computation \(K S^{-1} K^T\), where \(S \in \mathbb{R}^{M \times M}\).</li> <li>We set \(\mathcal{K} = K S^{-1} K^T\), and use that as our covariance matrix.</li> </ul> <p>Again, the log likelihood, as discussed, is given with our new covariance matrix and some added noise, as \(\mathcal{L} = \log p(y \vert X, \theta) = \log \mathcal{N} (y \vert 0, \mathcal{K} + \sigma^2 I)\).</p> <p>Simply expanding \(\mathcal{L}\) yields \(- \frac{1}{2} [ \log \vert 2 \pi (\mathcal{K} + \sigma^2 I) \vert + y^T (\mathcal{K} + \sigma^2I)^{-1} y ]\), which does not lead to any speedup in computation – we still have to perform \(N^3\) calculations to invert \(\Sigma\).</p> <p>However, we note that \((A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + V A^{-1}U)^{-1}VA^{-1}\), where A, U, C and V have the dimensions necessary for said operations. This is called the <em>Woodbury matrix identity</em>, and essentially says that the inverse of a rank-\(k\) correction to a matrix can be computed by doing a rank-\(k\) correction to the inverse of the original matrix.</p> <h3 id="aside-on-rank-corrections">Aside: on rank corrections.</h3> <p>While working on the Woodbury formula, we talked about <em>rank corrections</em> – what is that supposed to mean?</p> <p>Consider a linear regression problem, when you’re trying to compute the coefficients of a linear model. The closed form solution to a plain linear regression model is, \((X^T X)^{-1} X^T Y\). Assume that this solution has full-column rank, which means that the variables involved are linearly independent.</p> <p>What are we to do when we observe new samples, \(x_{n+1}, x_{n+2}, ... \in \mathbb{R}^k\)? Rather than explicitly re-computing the matrix inverse of \(M = (X^T X)\) wastefully, we use the matrix inversion lemma to update the value of this \(M\), which we should store somewhere when saving our model.</p> <h3 id="back-to-the-nyström-approximation">Back to the Nyström Approximation</h3> <p>We note here that the Woodbury formula reduces the term \((\mathcal{K} + \sigma^2 I)^{-1}\), which appears in the expansion for our Normal distribution, to \([\sigma^{-2}I - \sigma^{-4}K(S + \sigma^{-2}K^T K )^{-1}K^T]\).</p> <p>Note that here, \(K \in \mathbb{R}^{N \times M}\) and \(S \in \mathbb{R}^{M \times M}\).</p> <p>Consequently, the matrix inverted, \((S + \sigma^{-2}K^T K )\) is of size \(M \times M\).</p> <p>Immediately, our complexity falls to \(\mathcal{O}(NM^2)\). That is the Nyström Approximation, and our approximation gets better and better the closer we get to the whole dataset. The subset selection is generally done randomly.</p> ]]></content><author><name>Progyan Das</name></author><summary type="html"><![CDATA[Sparse, Variational, Gaussian, and how they can be combined in one process.]]></summary></entry></feed>